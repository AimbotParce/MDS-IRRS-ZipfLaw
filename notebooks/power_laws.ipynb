{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a1defe9-0d69-429b-9dcf-79cf72b20e31",
   "metadata": {
    "id": "1a1defe9-0d69-429b-9dcf-79cf72b20e31"
   },
   "source": [
    "# IRRS Lab Session 1: Powerlaws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efc4b56-ecee-4b48-bea1-8a3a28983a3a",
   "metadata": {
    "id": "8efc4b56-ecee-4b48-bea1-8a3a28983a3a"
   },
   "source": [
    "In this session you will:\n",
    "\n",
    "- Do simple preprocessing with `nltk`\n",
    "- Learn about power laws, a special case being Zipf's law\n",
    "- Investigate whether _Don Quijote_ follows Zipf and Herdan's power laws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c6b610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.10/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.10/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "# uncomment in case you need to install nltk\n",
    "!pip install nltk --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zx9JIvAz7j9U",
   "metadata": {
    "id": "zx9JIvAz7j9U"
   },
   "source": [
    "## 1. On preprocessing (with `nltk`)\n",
    "\n",
    "For this first project, you may use the very popular [natural language tooklit library](https://www.nltk.org) which provides useful language processing functionality.\n",
    "\n",
    "In what follows you can see some examples of the type of preprocessing that we will use during the course.\n",
    "\n",
    "- _Tokenization:_ is the process that splits a sequence of characters into a sequence of word types (or _tokens_)\n",
    "\n",
    "- _Lower case folding_: turning all characters to lowercase\n",
    "\n",
    "- _Stopword removal:_ removes tokens that correspond to functional words (or _stopwords_)\n",
    "\n",
    "- _Stemming:_ mapping word types to their stem (normalization)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ukKyUlRn_BHt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1694164723869,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -120
    },
    "id": "ukKyUlRn_BHt",
    "outputId": "a91aaf73-664e-4664-d3df-ec8519c6afc7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wLMn9Luk-1bN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1694164724102,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -120
    },
    "id": "wLMn9Luk-1bN",
    "outputId": "b1f58314-d2fa-4fed-a949-bb2b81034357"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Primera',\n",
      " 'parte',\n",
      " 'del',\n",
      " 'ingenioso',\n",
      " 'hidalgo',\n",
      " 'don',\n",
      " 'Quijote',\n",
      " 'de',\n",
      " 'la',\n",
      " 'Mancha',\n",
      " '.',\n",
      " 'Capitulo',\n",
      " 'primero',\n",
      " '.',\n",
      " 'Que',\n",
      " 'trata',\n",
      " 'de',\n",
      " 'la',\n",
      " 'condicion',\n",
      " 'y',\n",
      " 'ejercicio',\n",
      " 'del',\n",
      " 'famoso',\n",
      " 'hidalgo',\n",
      " 'don',\n",
      " 'Quijote',\n",
      " 'de',\n",
      " 'la',\n",
      " 'Mancha']\n"
     ]
    }
   ],
   "source": [
    "## tokenizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"\"\"Primera parte del ingenioso hidalgo don Quijote de la Mancha.\n",
    "  Capitulo primero. Que trata de la condicion y ejercicio del famoso hidalgo\n",
    "  don Quijote de la Mancha\"\"\"\n",
    "\n",
    "tokenized_text = word_tokenize(text)\n",
    "\n",
    "pprint(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o1TV4DnX_31F",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1694164724103,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -120
    },
    "id": "o1TV4DnX_31F",
    "outputId": "66d46fde-6284-4902-ad3c-7e1648910db1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 345 stopwords in Spanish (including punctuation), which are: {'estuvieras', 'hemos', 'tengamos', 'teniendo', 'tendrías', 'fueseis', 'son', 'las', 'estadas', 'tenidos', 'éramos', 'mis', 'estos', 'suya', 'durante', 'estoy', 'estuviese', 'hubiésemos', 'más', 'vosotros', 'estuvierais', 'estado', 'fueses', 'vuestras', 'y', 'porque', 'quien', 'habido', 'ha', 'tenidas', 'estuve', 'tendrían', 'habrás', 'lo', 'estuviésemos', 'tuvierais', 'estarás', 'la', 'a', 'por', 'habríais', 'será', 'nada', 'seremos', 'contra', 'mi', 'sean', 'como', 'seréis', 'estas', 'mía', 'estados', 'habían', 'eran', 'sentida', 'hubieseis', 'hayáis', 'seríais', '@', 'tendremos', 'somos', 'sois', 'estábamos', 'ellas', '%', 'estaríais', 'eso', 'hubieron', 'seas', 'habría', 'tenido', 'habrá', 'teníais', '/', 'hubisteis', 'estará', 'fue', 'estuvieseis', 'también', 'hasta', 'uno', 'me', 'habré', '_', 'al', 'tenían', 'tú', 'se', 'habréis', 'estaría', 'sería', 'los', 'habremos', 'tu', 'tuviéramos', 'estuvo', 'del', 'sí', 'tengáis', 'tuyas', 'les', 'has', 'mucho', 'estando', 'hubierais', 'tenida', 'haya', 'tienes', 'hube', 'de', 'tuyo', 'nos', 'otros', 'era', 'suyas', 'están', 'había', 'estuvieran', 'vuestra', 'algo', '`', 'tendré', 'estuvieron', 'hubiera', 'habidas', '^', 'hubimos', 'hubiese', 'tuviera', 'estáis', 'fuimos', 'eres', 'tendría', 'habríamos', 'tenemos', 'estás', '{', 'hubo', 'tienen', 'cual', 'habíais', 'serás', ',', 'tenía', 'hay', 'suyo', 'he', 'fueran', 'ya', 'estarían', 'nuestras', '&', 'nosotras', 'estar', 'vosotras', 'hayas', 'sentido', 'fueras', 'tendrá', 'tuviese', '=', '\\\\', 'mí', 'hubiste', 'habrían', 'seáis', 'siente', 'fui', 'estada', 'tengo', 'otra', 'ni', 'estéis', 'tuvieron', '-', 'sobre', 'todos', '~', 'tenga', 'sentidos', 'mías', 'tenías', 'habida', 'esas', 'estuviesen', 'es', 'tendréis', 'tuvisteis', 'tendrás', 'estuvisteis', 'pero', 'habiendo', 'teníamos', 'habéis', 'hayamos', 'estabas', 'tuyos', 'tuvieras', 'otras', 'este', 'habrán', 'estemos', 'todo', 'estaré', 'tuvieran', 'erais', 'sin', ')', 'para', 'esto', 'fuésemos', 'no', '$', 'algunas', \"'\", 'estarán', 'estarías', 'que', 'tuya', 'hubiesen', 'vuestro', 'seamos', 'tuve', '.', 'hubieran', 'entre', '#', 'esté', '|', 'míos', 'estuviéramos', 'sentidas', 'vuestros', 'estés', 'serías', 'fuese', 'os', 'muy', 'quienes', 'tiene', 'estaremos', 'te', 'han', 'estuvimos', 'seríamos', 'nuestros', 'seré', 'tuvieseis', 'tendríamos', 'estamos', 'le', 'estad', 'estuviera', 'tanto', 'él', 'tened', 'o', ':', 'habíamos', 'desde', 'estuviste', 'sea', '}', 'tengan', 'hubieses', 'tuvieses', 'serían', 'hubieras', 'fuerais', '*', '+', 'fuesen', 'tuviste', 'está', 'fuisteis', 'serán', '>', 'nuestro', 'ese', 'tuviésemos', 'tendrán', '<', 'tendríais', 'algunos', 'sus', 'suyos', 'estén', 'sintiendo', 'tengas', 'tuvo', 'esos', 'habrías', 'habías', 'habidos', 'estaban', 'donde', 'nuestra', 'estabais', 'ante', ']', 'soy', 'otro', 'tuviesen', 'estuvieses', 'con', 'hayan', 'estaréis', '(', 'fuera', 'fuiste', 'ti', 'eras', 'un', 'esa', 'mío', 'fuéramos', 'unos', 'en', 'tuvimos', 'qué', 'ella', 'una', 'tus', '?', '[', 'el', 'poco', 'antes', ';', 'tenéis', 'fueron', 'estaba', 'ellos', 'e', '!', 'muchos', 'estaríamos', 'su', 'nosotros', 'sentid', 'cuando', 'hubiéramos', 'esta', '\"', 'yo'}\n",
      "['primera',\n",
      " 'parte',\n",
      " 'ingenioso',\n",
      " 'hidalgo',\n",
      " 'don',\n",
      " 'quijote',\n",
      " 'mancha',\n",
      " 'capitulo',\n",
      " 'primero',\n",
      " 'trata',\n",
      " 'condicion',\n",
      " 'ejercicio',\n",
      " 'famoso',\n",
      " 'hidalgo',\n",
      " 'don',\n",
      " 'quijote',\n",
      " 'mancha']\n"
     ]
    }
   ],
   "source": [
    "## stopword removal, punctuation removal, lower-case folding\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "spanish_sw = set(stopwords.words('spanish') + list(string.punctuation))\n",
    "\n",
    "print(f'there are {len(spanish_sw)} stopwords in Spanish (including punctuation), which are: {spanish_sw}')\n",
    "\n",
    "filtered_tokenized_text = [w.lower() for w in tokenized_text if w.lower() not in spanish_sw]\n",
    "pprint(filtered_tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MYrK2m-oCw-0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1694164724103,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -120
    },
    "id": "MYrK2m-oCw-0",
    "outputId": "88b2cbfb-7ba1-496f-ad96-e7be38bfc26e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['primer',\n",
      " 'part',\n",
      " 'ingeni',\n",
      " 'hidalg',\n",
      " 'don',\n",
      " 'quijot',\n",
      " 'manch',\n",
      " 'capitul',\n",
      " 'primer',\n",
      " 'trat',\n",
      " 'condicion',\n",
      " 'ejercici',\n",
      " 'famos',\n",
      " 'hidalg',\n",
      " 'don',\n",
      " 'quijot',\n",
      " 'manch']\n"
     ]
    }
   ],
   "source": [
    "## stemmer\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "stemmed_text = [stemmer.stem(w) for w in filtered_tokenized_text]\n",
    "pprint(stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zHWeXKgYDlFz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1694164724302,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -120
    },
    "id": "zHWeXKgYDlFz",
    "outputId": "6a347f08-b005-4444-eff7-b9449bac6e3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'primer': 2,\n",
      "         'hidalg': 2,\n",
      "         'don': 2,\n",
      "         'quijot': 2,\n",
      "         'manch': 2,\n",
      "         'part': 1,\n",
      "         'ingeni': 1,\n",
      "         'capitul': 1,\n",
      "         'trat': 1,\n",
      "         'condicion': 1,\n",
      "         'ejercici': 1,\n",
      "         'famos': 1})\n",
      "Counter({'hidalgo': 2,\n",
      "         'don': 2,\n",
      "         'quijote': 2,\n",
      "         'mancha': 2,\n",
      "         'primera': 1,\n",
      "         'parte': 1,\n",
      "         'ingenioso': 1,\n",
      "         'capitulo': 1,\n",
      "         'primero': 1,\n",
      "         'trata': 1,\n",
      "         'condicion': 1,\n",
      "         'ejercicio': 1,\n",
      "         'famoso': 1})\n"
     ]
    }
   ],
   "source": [
    "## useful for counting words\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "word_counts = Counter(stemmed_text)\n",
    "pprint(word_counts)\n",
    "\n",
    "word_counts_no_stem = Counter(filtered_tokenized_text)\n",
    "pprint(word_counts_no_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b5cdc6-8e9c-47e7-8f8d-bbb5ab5d9b57",
   "metadata": {
    "id": "82b5cdc6-8e9c-47e7-8f8d-bbb5ab5d9b57"
   },
   "source": [
    "## 2. On powerlaws\n",
    "\n",
    "Consider a function $y=f(x)$, that we will call a \"law\" relating $y$ with $x$.\n",
    "\n",
    "__Example 1.__ You are given a bucket of some radioactive isotope. $x$ is the number of seconds since you were given\n",
    "the bucket. $y$ is the number of atoms that disintegrate at the $x$th second.\n",
    "Each atom decides independently from all other atoms whether to disintegrate in the next second (or\n",
    "nanosecond, or whatever). From here you can see that $f$ will have the form $y=c \\cdot x^{-a}$,\n",
    "where $c$ depends on the number of atoms you were given, and $a$ depends on the isotope. In particular,\n",
    "$a>1$ determines the half-life of the isotope.\n",
    "\n",
    "__Example 2.__ $x$ is number of seconds and $y$ is the number of people entering a metro station during the next\n",
    "10 minutes. You may have been told in statistics that $f$ is given by the Poisson distribution\n",
    "(at least for ideal people and metro stations).\n",
    "\n",
    "Many natural and artificial phenomena come in distributions that are neither exponential nor Poisson,\n",
    "but so-called power laws. Intuitively, we have $y$ evolves like $x^a$, where $a$ is a constant called\n",
    "the _exponent_ of the power law. If $a$ is positive, $y$ is increasing, and if $a$ is negative,\n",
    "$y$ decreases.\n",
    "\n",
    "More precisely, a power law is\n",
    "$$\n",
    "y = c \\cdot (x+b)^a\n",
    "$$\n",
    "for three constants $a$, $b$, and $c$.\n",
    "\n",
    "There is a lot of theory about why powerlaws are ubiqutous. They are often related to self-organization,\n",
    "fractality, complex dynamical systems, etc. The Web and social networks are full of powerlaws.\n",
    "So is human language, social sciences (population of cities), and natural sciences (intensity of earthquakes,\n",
    "relation of size and metabolism in animals).\n",
    "\n",
    "A powerlaw with exponent $-1$ is called Zipf's law, after Mr. Zipf. By extension, sometimes powerlaws with\n",
    "negative exponents are also called Zipfian laws.\n",
    "\n",
    "In what follows we will fit some datasets to the best powerlaw that we can find - and see that they pretty close.\n",
    "Note that this does NOT mean that the phenomenon generating the dataset is exactly a powerlaw.\n",
    "Proving seriously that a law is (or is not) a powerlaw is another matter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c75cda0-0d03-4fdc-9aa0-785d5232354b",
   "metadata": {
    "id": "5c75cda0-0d03-4fdc-9aa0-785d5232354b"
   },
   "source": [
    "## 3. Looking at \"Don Quijote\"\n",
    "\n",
    "In this exercise, we will be looking at Zipf's and Heap's text laws of the classical text _Don Quijote_. The full plain text is publicly available in many places. You should do a basic preprocessing before counting words. For this exercise, it is better if you _do not remove stopwords_ since we are trying to reflect the full word frequency distribution, including stopwords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c244af-17d7-4ad9-8515-feca1b3cea20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 692,
     "status": "ok",
     "timestamp": 1694164724991,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -120
    },
    "id": "10c244af-17d7-4ad9-8515-feca1b3cea20",
    "outputId": "7d3a3e7d-010f-4e26-d528-4231d61624ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primera parte del ingenioso hidalgo don Quijote de la Mancha\n",
      "Capítulo primero. Que trata de la condición y ejercicio del famoso hidalgo\n",
      "don Quijote de la Mancha\n",
      "En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho\n",
      "tiempo que vivía un hidalgo de los de lanza en astillero, adarga antigua,\n",
      "rocín flaco y galgo corredor. Una olla de algo más vaca que carnero,\n",
      "salpicón las más noches, duelos y quebrantos los sábados, lantejas los\n",
      "viernes, algún palomino de añadidura los domingos, consumían las tres\n",
      "partes de su hacienda. El resto della concluían sayo de velarte, calzas de\n",
      "velludo para las fiestas, con sus pantuflos de lo mesmo, y los días de\n"
     ]
    }
   ],
   "source": [
    "# load don quijote, e.g. as in:\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "target_url = \"https://fegalaz.usc.es/~gamallo/aulas/lingcomputacional/corpus/quijote-es.txt\"\n",
    "\n",
    "data = urllib.request.urlopen(target_url)\n",
    "\n",
    "# print first N lines only as a demo\n",
    "N = 10\n",
    "for _ in range(N):\n",
    "    line = next(data)\n",
    "    print(line.decode('latin-1').strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fba2952-4558-4c64-a228-7b8d2a07d513",
   "metadata": {
    "id": "9fba2952-4558-4c64-a228-7b8d2a07d513"
   },
   "source": [
    "---\n",
    "\n",
    "__Exercise 1.__ Use `python` + `matplotlib` or similar library to plot the frequence of words in `don quijote` in decreasing order.\n",
    "\n",
    "---\n",
    "\n",
    "Is it a powerlaw? Or, can it be approximated by a powerlaw?\n",
    "\n",
    "A trick about powerlaws is the following. Let's forget about the $b$ parameter for a second (or equivalently assume that $b = 0$),\n",
    "so our powerlaw looks like\n",
    "$$\n",
    "y = c \\cdot x^a.\n",
    "$$\n",
    "\n",
    "Taking logs on both sides, it becomes\n",
    "$$\n",
    "\\log y = a \\cdot \\log x + \\log c\n",
    "$$\n",
    "\n",
    "I.e., $\\log y$ is a linear function of $\\log x$.\n",
    "\n",
    "---\n",
    "\n",
    "__Exercise 2.__ Now, plot the same but use _logarithmic_ $x$ and $y$ axes.\n",
    "\n",
    "---\n",
    "\n",
    "If our distribution is a powerlaw, this plot should be a straight line, whose slope is $a$\n",
    "and intercept is $\\log c$. If we put back the $b$ parameter, it distorts a bit the\n",
    "low values, so in order to estimate $a$ and $c$ we have to pay attention to the large values.\n",
    "\n",
    "__You can try different preprocessing steps to see whether the distribution is closer to a powerlaw or not. For example, if you do not stem you may obtain a distribution that resembles more a powerlaw.__\n",
    "\n",
    "---\n",
    "\n",
    "__Exercise 3.__ Let's find $a$ and $c$ analytically.\n",
    "Assume we have $\\log y = a \\cdot \\log x + \\log c$.\n",
    "\n",
    "Take two distinct _large_ values of $x$, find their corresponding values of $y$,\n",
    "set up a system of two linear equations, and solve for $a$ and $c$.\n",
    "The solution will probably not fit very well the low values of $x$. You can try to\n",
    "make it better by adding the $b$ parameter, but don't agonize over it.\n",
    "\n",
    "Alternatively, use linear regression to estimate the _slope_  ($a$ parameter) and the intercept ($\\log c$ parameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc3388f-4717-4175-b640-6b6a08fa310c",
   "metadata": {
    "id": "cfc3388f-4717-4175-b640-6b6a08fa310c"
   },
   "source": [
    "---\n",
    "\n",
    "__Exercise 4.__  Now, it is time to check whether Heap's law applies in the _Don Quijote_. First, plot number of different words (word _types_) as a function of text length. Next, plot it on a log-log scale. You should see a straight line. Finally, give an estimate of $\\beta$ parameter of Heap's law:\n",
    "\n",
    "$$  d = k \\cdot N^{\\beta} $$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "icodIUNAIoLP",
   "metadata": {
    "id": "icodIUNAIoLP"
   },
   "source": [
    "## 4. Rules of delivery\n",
    "\n",
    "- To be solved in _pairs_.\n",
    "\n",
    "- No plagiarism; don't discuss your work with other teams. You can ask for help to others for simple things, such as recalling a python instruction or module, but nothing too specific to the session.\n",
    "\n",
    "- If you feel you are spending much more time than the rest of the classmates, ask us for help. Questions can be asked either in person or by email, and you'll never be penalized by asking questions, no matter how stupid they look in retrospect.\n",
    "\n",
    "- Write a short report listing the solutions to the exercises proposed. Include things like: how did you find the best $a$, $b$, $c$ parameter values for the text laws? Please include plots that compare the fitted powerlaws with the real data (preferably in log-log scale). You are welcome to add conclusions and findings that depart from what we asked you to do. We encourage you to discuss the difficulties you find; this lets us give you help and also improve the lab session for future editions.\n",
    "\n",
    "- Turn the report to PDF. Make sure it has your names, date, and title.\n",
    "\n",
    "- Submit your work through the [raco](http://www.fib.upc.edu/en/serveis/raco.html). There will be a `Practicals` open for each report.\n",
    "\n",
    "- Deadline: Work must be delivered __within 2 weeks__ from the end of the lab session. Late submissions risk being penalized or not accepted at all. If you anticipate problems with the deadline, tell us as soon as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pjYVroT2IwdU",
   "metadata": {
    "id": "pjYVroT2IwdU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "zipf-law",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
